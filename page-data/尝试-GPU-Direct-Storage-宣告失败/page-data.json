{"componentChunkName":"component---src-templates-blog-post-js","path":"/尝试-GPU-Direct-Storage-宣告失败/","result":{"data":{"markdownRemark":{"html":"<p>这两天我突然发掘了 <a href=\"https://developer.nvidia.com/blog/gpudirect-storage/\">GPU Direct Storage</a> （后文都称 gds）这个东西，他可以允许从 GPU 显存到 NVMe 或支持 RDMA 的文件系统的 DMA。虽然好像是在 19 年就发布了，但是在 cuda 11.4 之前，这个东西主要是应用在 RAPDIS 里面，例如 <a href=\"https://nvidia.github.io/spark-rapids/docs/additional-functionality/gds-spilling.html\">spark-rapids</a> 中。cuda 11.4 中把这部分放进 cuda 了，也终于标志成了 1.0。</p>\n<p>而且 GPU Storage 也退出了一个非常方便的接口：cuFile。可以用下面两段代码从文件读取数据到 GPU 的代码对比一下。</p>\n<p>POSIX API：</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">int</span> fd <span class=\"token operator\">=</span> <span class=\"token function\">open</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>sysmem_buf<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>gpumem_buf<span class=\"token punctuation\">;</span>\nsysmem_buf <span class=\"token operator\">=</span> <span class=\"token function\">malloc</span><span class=\"token punctuation\">(</span>buf_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token function\">cudaMalloc</span><span class=\"token punctuation\">(</span>gpumem_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token function\">pread</span><span class=\"token punctuation\">(</span>fd<span class=\"token punctuation\">,</span> sysmem_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>sysmem_buf<span class=\"token punctuation\">,</span> \n  gpumem_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> H2D<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> \ndoit<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>gpumem_buf<span class=\"token punctuation\">,</span> …<span class=\"token operator\">>></span><span class=\"token operator\">></span> </code></pre></div>\n<p>cuFile API：</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">int</span> fd <span class=\"token operator\">=</span> <span class=\"token function\">open</span><span class=\"token punctuation\">(</span>file_name<span class=\"token punctuation\">,</span> O_DIRECT<span class=\"token punctuation\">,</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span>\nCUFileHandle_t <span class=\"token operator\">*</span>fh<span class=\"token punctuation\">;</span> \nCUFileDescr_t desc<span class=\"token punctuation\">;</span> \ndesc<span class=\"token punctuation\">.</span>type<span class=\"token operator\">=</span>CU_FILE_HANDLE_TYPE_OPAQUE_FD<span class=\"token punctuation\">;</span>\ndesc<span class=\"token punctuation\">.</span>handle<span class=\"token punctuation\">.</span>fd <span class=\"token operator\">=</span> fd<span class=\"token punctuation\">;</span>\n<span class=\"token function\">cuFileHandleRegister</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>fh<span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>desc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>gpumem_buf<span class=\"token punctuation\">;</span>\n<span class=\"token function\">cudaMalloc</span><span class=\"token punctuation\">(</span>gpumem_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token function\">cuFileRead</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>fh<span class=\"token punctuation\">,</span> gpumem_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> …<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\ndoit<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>gpumem_buf<span class=\"token punctuation\">,</span> …<span class=\"token operator\">>></span><span class=\"token operator\">></span> </code></pre></div>\n<p>可以看到 cufile 使用起来还是非常自然的。所以我本来计划得可好了，既然 gds 可以将硬盘文件直接传到 GPU ，可以做一个 gds 版本的 embedding，在 CPU 中记录 feature key 在硬盘文件的位置，在硬盘上记录实际的 embedding。这样至少可以提升在推荐场景下单机能够承载的 embedding 大小。如果想做的更复杂一些，没准可以整一个结合 GPU 的 lmdb（B 树）。这里想用 B 树是因为感觉 LSM 那种形式和 embedding 有些不搭。</p>\n<p>不过这些美好的计划很快被现实击碎了。</p>\n<p>首先是装环境，官方表示只有在 ubuntu 18.04, 20.04 或者 RHEL 8.4 上才有支持的 cuda，并且还不能用 <code class=\"language-text\">.run</code> 文件，而是要用下面这样的方式安装：</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n<span class=\"token function\">sudo</span> <span class=\"token function\">mv</span> cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n<span class=\"token function\">wget</span> https://developer.download.nvidia.com/compute/cuda/11.5.0/local_installers/cuda-repo-ubuntu2004-11-5-local_11.5.0-495.29.05-1_amd64.deb\n<span class=\"token function\">sudo</span> dpkg <span class=\"token parameter variable\">-i</span> cuda-repo-ubuntu2004-11-5-local_11.5.0-495.29.05-1_amd64.deb\n<span class=\"token function\">sudo</span> apt-key <span class=\"token function\">add</span> /var/cuda-repo-ubuntu2004-11-5-local/7fa2af80.pub\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> update\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token parameter variable\">-y</span> <span class=\"token function\">install</span> cuda</code></pre></div>\n<p>因为公司内网下载外链极慢，所以整了快一天才下下来。结果发现 <code class=\"language-text\">apt install cuda</code> 是会伴随着指定的 driver 版本的。具体来说是会装一个 495 版本的驱动...公司里的驱动版本自然是没有那么高的，所以这个安装直接把镜像搞崩了，连 <code class=\"language-text\">nvidia-smi</code> 都跑不了了。</p>\n<p>不过好在，我直接跑到 cuda 目录下头，把 <code class=\"language-text\">cufile.h</code> 以及 <code class=\"language-text\">libcufile.so</code> 搞出来了。用他们在以前的 cuda 11.0 的镜像里面编译是可以运行的。</p>\n<p>虽然能跑了，但是实际上因为并没有装上指定的环境，所以按照文档说的，cufile 会运行兼容模式，就还是先分配 pin memory buffer，然后一点一点拷贝...</p>\n<blockquote>\n<p>Although the purpose of GDS is to avoid using a bounce buffer in CPU system memory, the ability to fall back to this approach allows the cuFile APIs to be used ubiquitously even under suboptimal circumstances. A compatibility mode is available for unsupported configurations that maps IO operations to a fallback path.</p>\n<p>This path stages through CPU system memory for systems where one or more of the following conditions is true:</p>\n<ul>\n<li>\n<p>Explicit configuration control by using the user version of the <code class=\"language-text\">cufile.json</code> file.</p>\n</li>\n<li>\n<p>The lack of availability of the <code class=\"language-text\">nvidia-fs.ko</code> kernel driver, for example, because it was not installed on the host machine, where a container with an application that uses cuFile, is running.</p>\n</li>\n<li>\n<p>The lack of availability of relevant GDS-enabled filesystems on the selected file mounts, for example, because one of several used system mounts does not support GDS.</p>\n</li>\n<li>\n<p>File-system-specific conditions, such as when <code class=\"language-text\">O_DIRECT</code> cannot be applied.</p>\n</li>\n</ul>\n</blockquote>\n<p>为了不使用兼容模式，需要安装一个 <code class=\"language-text\">nvidia-fs.ko</code> kernel driver。所以其实到这里我的应用之路就卡死了。因为我们的应用都是在镜像中进行的，是没法装 kernel module 的。之前薛磊大大的 fgpu 也是用的同样的技术，也是需要用母机部署。</p>\n<p>但是既然时间已经花出去了，直接到这里结束就太可惜了。所以不如继续研究几个问题。</p>\n<ul>\n<li>为啥镜像里面不能配置 kernel？</li>\n</ul>\n<p>因为 docker 里的容器其实就是一个进程，所以其一直是用的是 host kernel，syscall 也会直接走到 host kernel 去。而 container 里是没有 kernel 或者 kernel module 的。所以在 container 里面注册也并不会注册上。</p>\n<ul>\n<li>gds 为啥需要装一个 kernel module？为啥 gds 只能从 NVMe 文件读数据，或者是从支持 RDMA 的分布式文件系统中读数据？</li>\n</ul>\n<p>下图是 gds 的一个结构：</p>\n<p><img src=\"/blog/img/gds-arch-overview.png\" alt=\"\"></p>\n<p>可以看到 cuFile 的实现方法是像 kernel module 发只有它能识别的 ioctl（ioctl 应该是 VFS 的一部分，所以派发 ioctl 应该是 VFS 做的），然后由这个 <code class=\"language-text\">nvidia-fs.ko</code> 实现从文件系统到显存的 DMA。</p>\n<p>实际上，nv 也开源了这个 kernel module：<a href=\"https://github.com/NVIDIA/gds-nvidia-fs\">github 链接</a>。所以我也偷偷尝试去编译了一下，结果发现他是需要 <code class=\"language-text\">/usr/src/nvidia-*</code> 这个位置的 driver 源码的。这种东西在镜像里是得不到的，所以也就没有办法了。这里我不太了解 <code class=\"language-text\">/usr/src/</code> 目录里到底是有啥，毕竟众所周知 nv 的驱动是闭源的，所以对应的目录里面肯定不是源码，可能是类似于 <code class=\"language-text\">.so</code> 的东西吧。</p>\n<h3>参考资料</h3>\n<ol>\n<li><a href=\"https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html\">https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html</a></li>\n<li><a href=\"https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-storage-partnerships/\">https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-storage-partnerships/</a></li>\n</ol>","frontmatter":{"title":"尝试 GPU Direct Storage 宣告失败","date":"2021-11-19T12:41:00.000Z","tags":["OS"]}}},"pageContext":{"slug":"/尝试-GPU-Direct-Storage-宣告失败/"}},"staticQueryHashes":["3159585216"],"slicesMap":{}}