{"componentChunkName":"component---src-templates-blog-post-js","path":"/关于大模型的一些思考/","result":{"data":{"markdownRemark":{"html":"<p>随着 llama3 的公布（以及 phi3 的 llm-arena 分数的公布），我认为现阶段大模型的格局已经比较明确了，即，</p>\n<ul>\n<li>llama3 使用的超多 pretrain 数据 + 适当的数据清洗的方案可以基本达到 GPT-4 级别模型的水平，是当前的开源版本答案。如果 llama3 的论文以及其 400B 模型会如实发布，那么大家的各种技术能力会被在这个层面上强制拉平，回到了拼硬件资源的状态；</li>\n<li>llama3 8B 展现了超过 GPT 3.5 的水平。这个水平的模型配合适当的 RAG 已经足以在端侧完成 80% 以上目前设想的 2C llm 应用了。所以这部分 llm 应用的商业价值基本是被清空了的。</li>\n</ul>\n<p>基于这样的一个现状。从乐观的角度来说，llm 走向千家万户的时间点被大大拉近了，我预估最晚明年年底，大部分旗舰手机中就一定会包含 7B 级别的 llm 支持了，各个 app 内部也会加入符合其特色的大模型应用。很难想象之后的用户会为了使用某个免费的大模型而在手机上下载某一家大模型应用的 app 了。</p>\n<p>但是从技术人员的角度来说，当美国开源了原子弹的制作方法时，各国的核弹研究人员就要各谋出路了。对于这样的出路，我认为主要取决于，是选择相信大模型的 scaling law 会延展到无比强大的超人水平，还是认为实际应用的大模型会长期处于 GPT-4 左右的类人水平。前者自然是 OpenAI 等公司的立身之本，他们也会持续极尽算力来在这个方向上做出突破。而对于我这样没办法得到充足算力支持的大厂工程人员来说，至少在 GPT-5 出来之前，我更愿意保守地认为我们的模型会处于 GPT-4 附近的水平（并迫切地希望 OpenAI 能再次发出让大家震惊的技术产品），也就是在各个领域都处于一个人类 junior 的水平。</p>\n<p>进一步来考虑，我们能做的就是如何利用好这种廉价的通用 junior。在能力的天花板基本能被探到的情况下，我们就要回到常规的 trade-off，性能和资源的 trade-off，通用和特化的 trade-off 等等。可以认为这条赛道上，我们不必再一味追求单模型端到端处理所有内容，而是要考虑如何复用传统的管理模式来管理大量的 llm，以及如何优化每个 llm 的成本。</p>\n<p>更细化地说，在工程上，优化成本即做如量化、剪枝、专项 finetune 等工作，复用管理方式即尽量将团队中人进行的工作转为 llm 的自动化工作（llm 是员工，人是领导）。我认为这是我们下一个阶段值得开展的工作。</p>","frontmatter":{"title":"关于大模型的一些思考","date":"2024-04-27T21:30:00.000Z","tags":["随感"]}}},"pageContext":{"slug":"/关于大模型的一些思考/"}},"staticQueryHashes":["3159585216"]}