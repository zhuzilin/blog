{"componentChunkName":"component---src-templates-blog-post-js","path":"/OSDI-2020-PipeSwitch-阅读笔记/","webpackCompilationHash":"d3c4e6ec70b03f15d22e","result":{"data":{"markdownRemark":{"html":"<p>这个项目的代码公布在了：<a href=\"https://github.com/netx-repo/PipeSwitch\">https://github.com/netx-repo/PipeSwitch</a> 和 <a href=\"https://github.com/Myrmustin/PipeSwitch_Plus\">https://github.com/Myrmustin/PipeSwitch_Plus</a></p>\n<p>虽然不知道 plus 版本是不是进阶版，但是还是看的 plus 版的。</p>\n<p>对于 C++ 部分，主要加入了 4 个函数：<code class=\"language-text\">allocateSharedCache</code>、<code class=\"language-text\">sendSharedCache</code>、<code class=\"language-text\">recvSharedCache</code>、<code class=\"language-text\">insertSharedCache</code> 和 <code class=\"language-text\">clearSharedCache</code>。</p>\n<h2>C++ 代码</h2>\n<h3>allocateSharedCache</h3>\n<p>就是分配一段显存在 <code class=\"language-text\">PIPESWITCH_shared_ptr</code>，大小为 12GB。</p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">#define SIZE_SHARED_CACHE (12 * 1024UL * 1024UL * 1024UL) // PipeSwitch\n\n  /* PipeSwitch: allocate shared GPU memory */\n  void allocateSharedCache() {\n    std::lock_guard&lt;std::recursive_mutex&gt; lock(mutex);\n    cudaError_t err = cudaMalloc(&amp;PIPESWITCH_shared_ptr, SIZE_SHARED_CACHE);\n    if (err != cudaSuccess) {\n      perror(&quot;allocate_shared_cache&quot;); \n      exit(EXIT_FAILURE); \n    }\n  }</code></pre></div>\n<h3>sendSharedCache</h3>\n<p>获取一个跨进程的 handle，并把这个 handle 发出去。</p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">  /* PipeSwitch: send shared GPU memory */\n  void sendSharedCache() {\n    std::lock_guard&lt;std::recursive_mutex&gt; lock(mutex);\n    cudaIpcMemHandle_t shared_cache_handle;\n\n    // Pack CUDA pointer\n    cudaError_t err = cudaIpcGetMemHandle(&amp;shared_cache_handle, PIPESWITCH_shared_ptr);\n    if (err != cudaSuccess) {\n      perror(&quot;pack_shared_cache&quot;); \n      exit(EXIT_FAILURE); \n    }\n\n    // Accept connection\n    int server_fd, conn_fd, valread; \n    int opt = 1;\n    struct sockaddr_in address; \n    int addrlen = sizeof(address);\n    if ((server_fd = socket(AF_INET, SOCK_STREAM, 0)) == 0) { \n        perror(&quot;socket failed&quot;); \n        exit(EXIT_FAILURE); \n    } \n    if (setsockopt(server_fd, SOL_SOCKET, SO_REUSEADDR | SO_REUSEPORT, &amp;opt, sizeof(opt))) { \n        perror(&quot;setsockopt&quot;); \n        exit(EXIT_FAILURE); \n    } \n    address.sin_family = AF_INET; \n    address.sin_addr.s_addr = INADDR_ANY; \n    address.sin_port = htons( PORT ); \n    if (bind(server_fd, (struct sockaddr *)&amp;address,  sizeof(address)) &lt; 0) { \n        perror(&quot;bind failed&quot;); \n        exit(EXIT_FAILURE); \n    }\n    if (listen(server_fd, 1) &lt; 0) { \n        perror(&quot;listen&quot;); \n        exit(EXIT_FAILURE); \n    } \n    if ((conn_fd = accept(server_fd, (struct sockaddr *)&amp;address, (socklen_t*)&amp;addrlen)) &lt; 0) { \n        perror(&quot;accept&quot;); \n        exit(EXIT_FAILURE); \n    }\n\n    // Send the packed pointer\n    write(conn_fd, (void*)(&amp;shared_cache_handle), sizeof(cudaIpcMemHandle_t));\n\n    close(conn_fd);\n    close(server_fd);\n  }</code></pre></div>\n<h3>recvSharedCache</h3>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">  /* PipeSwitch: recv shared GPU memory */\n  void recvSharedCache() {\n    std::lock_guard&lt;std::recursive_mutex&gt; lock(mutex);\n    cudaIpcMemHandle_t shared_cache_handle;\n\n    // Connect\n    int conn_fd = 0; \n    struct sockaddr_in serv_addr; \n    if ((conn_fd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0) { \n        printf(&quot;\\n Socket creation error \\n&quot;); \n        exit(EXIT_FAILURE); \n    }\n    serv_addr.sin_family = AF_INET; \n    serv_addr.sin_port = htons(PORT); \n    if(inet_pton(AF_INET, &quot;127.0.0.1&quot;, &amp;serv_addr.sin_addr) &lt;= 0) { \n        printf(&quot;\\nInvalid address/ Address not supported \\n&quot;); \n        exit(EXIT_FAILURE); \n    }\n    if (connect(conn_fd, (struct sockaddr *)&amp;serv_addr, sizeof(serv_addr)) &lt; 0) { \n        printf(&quot;\\nConnection Failed \\n&quot;); \n        exit(EXIT_FAILURE); \n    }\n\n    // Receive packed pointer\n    read(conn_fd, (void*)(&amp;shared_cache_handle), sizeof(cudaIpcMemHandle_t)); \n    \n    // Extract the pointer\n    cudaError_t err = cudaIpcOpenMemHandle(\n      \t&amp;PIPESWITCH_shared_ptr, shared_cache_handle, cudaIpcMemLazyEnablePeerAccess);\n    if (err != cudaSuccess) {\n      perror(&quot;extract_shared_cache&quot;); \n      exit(EXIT_FAILURE); \n    }\n\n    close(conn_fd);\n  }</code></pre></div>\n<h3>insertSharedCache</h3>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">  /* PipeSwitch: insert shared GPU memory to large block pool */\n    void insertSharedCache(size_t size, size_t offset) {\n    std::lock_guard&lt;std::recursive_mutex&gt; lock(mutex);\n    int device;\n    C10_CUDA_CHECK(cudaGetDevice(&amp;device));\n    Block* block = new Block(\n      device, \n      cuda::getCurrentCUDAStream(device), \n      size, \n      &amp;large_blocks, \n      static_cast&lt;char*&gt;(PIPESWITCH_shared_ptr) + offset);\n    // allocated_size += size;\n    large_blocks.insert(block);\n\n    get_stats_for_device(device).increaseCached(size);\n    return;\n  }</code></pre></div>\n<h3>clearSharedCache</h3>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">  /* PipeSwitch: clear shared GPU memory */\n  void clearSharedCache() {\n    std::lock_guard&lt;std::recursive_mutex&gt; lock(mutex);\n\n    int device;\n    C10_CUDA_CHECK(cudaGetDevice(&amp;device));\n    cudaStream_t stream = cuda::getCurrentCUDAStream(device);\n\n    std::cout &lt;&lt; &quot;Begin Clear&quot; &lt;&lt; std::endl;\n    auto it = large_blocks.begin();\n    while (it != large_blocks.end()) {\n      Block* block = *it;\n      // 这里没有 prev 没有 next 说明是没有进行切分的\n      // 同时 stream 要是 PipeSwitch 里面设置的 cuda_stream_for_parameter\n      if (block-&gt;stream == stream &amp;&amp; !block-&gt;prev &amp;&amp; !block-&gt;next) {\n        std::cout &lt;&lt; &quot;Clear&quot; &lt;&lt; &quot;, &quot; &lt;&lt; block-&gt;ptr &lt;&lt; &quot;, &quot;\n                  &lt;&lt; block-&gt;size &lt;&lt; &quot;, &quot; &lt;&lt; block-&gt;allocated &lt;&lt; std::endl;\n        const auto&amp; stats = get_stats_for_device(block-&gt;device);\n        get_stats_for_device(block-&gt;device).decreaseCached(block-&gt;size);\n\n        auto cur = it;\n        ++it;\n        large_blocks.erase(cur);\n        delete block;\n      }\n      else {\n        ++it;\n      }\n    }\n    std::cout &lt;&lt; &quot;End Clear&quot; &lt;&lt; std::endl;\n    // allocated_size = 0;\n  }</code></pre></div>\n<h3>kSmallSize</h3>\n<p>除此之外，PipeSwitch 还把 <code class=\"language-text\">kSmallSize</code> 设置为了 1。这意味着这样几件事：</p>\n<ul>\n<li><code class=\"language-text\">get_pool</code> 永远使用 <code class=\"language-text\">large_blocks</code>；</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">  BlockPool&amp; get_pool(size_t size) {\n    if (size &lt;= kSmallSize) {\n      return small_blocks;\n    } else {\n      return large_blocks;\n    }\n  }</code></pre></div>\n<p>在 <code class=\"language-text\">THCCachingAllocator</code> 中，有  <code class=\"language-text\">large_blocks</code> 和 <code class=\"language-text\">small_blocks</code> 两个 <code class=\"language-text\">BlockPool</code>（其实就是 <code class=\"language-text\">std::set</code>）。内部 block 的排序方式如下：</p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">static bool BlockComparator(const Block* a, const Block* b)\n{\n  if (a-&gt;device != b-&gt;device) {\n    return a-&gt;device &lt; b-&gt;device;\n  }\n  if (a-&gt;stream != b-&gt;stream) {\n    return (uintptr_t)a-&gt;stream &lt; (uintptr_t)b-&gt;stream;\n  }\n  if (a-&gt;size != b-&gt;size) {\n    return a-&gt;size &lt; b-&gt;size;\n  }\n  return (uintptr_t)a-&gt;ptr &lt; (uintptr_t)b-&gt;ptr;\n}</code></pre></div>\n<p>这里比较上的一个重点在于先比较 device，再比较 stream，最后再找合适的 size。而在具体查找 block 的时候，更是只能允许选择同一个 stream 上的内存：</p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">    auto find_free_block = [&amp;]()-&gt;Block*{\n      auto it = pool.lower_bound(&amp;search_key);\n      if (it != pool.end() &amp;&amp; (*it)-&gt;device == device &amp;&amp;\n          (*it)-&gt;stream == stream) {\n        Block* block = *it;\n        pool.erase(it);\n        return block;\n      }\n      return nullptr;\n    };</code></pre></div>\n<p>这个机制使得 PipeSwitch 可以在不同的 stream 里面共用同一段显存。</p>\n<ul>\n<li>对于 <code class=\"language-text\">large_blocks</code>，只要有剩余，就会拆分；</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">  bool should_split(Block* block, size_t size) {\n    size_t remaining = block-&gt;size - size;\n    if (block-&gt;pool == &amp;small_blocks) {\n      return remaining &gt;= kMinBlockSize;\n    } else if (block-&gt;pool == &amp;large_blocks) {\n      // 对于 large blocks，如果有 remaining，就一定拆分\n      return remaining &gt; kSmallSize;\n    } else {\n      AT_ERROR(&quot;should_split: invalid pool&quot;);\n    }\n  }</code></pre></div>\n<p>因为由上面一点，知道所有的 block 都在 <code class=\"language-text\">large_blocks</code> 里面，所以可以说所有的 block 都会继续拆分（感觉这样会出现比较严重的 fragmentation）。</p>\n<ul>\n<li>永远不会用 <code class=\"language-text\">kSmallBuffer</code></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">  size_t get_allocation_size(size_t size) {\n    if (size &lt;= kSmallSize) {\n      return kSmallBuffer;  // 2 MB\n    } else if (size &lt; kMinLargeAlloc) {\n      return kLargeBuffer;  // 20 MB\n    } else {\n      // 2N MB\n      return kRoundLarge * ((size + kRoundLarge - 1) / kRoundLarge);\n    }\n  }</code></pre></div>\n<h2>python 代码</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># PipeSwitch</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">allocate_shared_cache</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> _initialized<span class=\"token punctuation\">:</span>\n        torch<span class=\"token punctuation\">.</span>_C<span class=\"token punctuation\">.</span>_cuda_allocateSharedCache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># PipeSwitch</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">send_shared_cache</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> _initialized<span class=\"token punctuation\">:</span>\n        torch<span class=\"token punctuation\">.</span>_C<span class=\"token punctuation\">.</span>_cuda_sendSharedCache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># PipeSwitch</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">recv_shared_cache</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> _initialized<span class=\"token punctuation\">:</span>\n        torch<span class=\"token punctuation\">.</span>_C<span class=\"token punctuation\">.</span>_cuda_recvSharedCache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># PipeSwitch</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">insert_shared_cache_for_parameter</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> _initialized<span class=\"token punctuation\">:</span>\n        torch<span class=\"token punctuation\">.</span>_C<span class=\"token punctuation\">.</span>_cuda_insertSharedCacheForParameter<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># PipeSwitch</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">insert_shared_cache_for_computation</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> _initialized<span class=\"token punctuation\">:</span>\n        torch<span class=\"token punctuation\">.</span>_C<span class=\"token punctuation\">.</span>_cuda_insertSharedCacheForComputation<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># PipeSwitch</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">clear_shared_cache</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> _initialized<span class=\"token punctuation\">:</span>\n        torch<span class=\"token punctuation\">.</span>_C<span class=\"token punctuation\">.</span>_cuda_clearSharedCache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>这里可能仅仅需要分辨的是 insert for parameter 和 insert for computation 了：</p>\n<div class=\"gatsby-highlight\" data-language=\"c++\"><pre class=\"language-c++\"><code class=\"language-c++\">// PipeSwitch\nPyObject * THCPModule_insertSharedCacheForParameter(PyObject *_unused, PyObject *noargs)\n{\n  HANDLE_TH_ERRORS\n      c10::cuda::CUDACachingAllocator::insertSharedCache(\n    \t\t\t1UL * 1024UL * 1024UL * 1024UL, 0);\n  END_HANDLE_TH_ERRORS\n  Py_RETURN_NONE;\n}\n\n// PipeSwitch\nPyObject * THCPModule_insertSharedCacheForComputation(PyObject *_unused, PyObject *noargs)\n{\n  HANDLE_TH_ERRORS\n      c10::cuda::CUDACachingAllocator::insertSharedCache(\n    \t\t\t11UL * 1024UL * 1024UL * 1024UL, 1UL * 1024UL * 1024UL * 1024UL);\n  END_HANDLE_TH_ERRORS\n  Py_RETURN_NONE;\n}</code></pre></div>\n<p>相当于就是前面 1G 用作参数，后面 11G 用作计算。</p>","frontmatter":{"title":"OSDI 2020 PipeSwitch 阅读笔记","date":"2021-11-02T21:30:00.000Z","tags":["paper"]}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/OSDI-2020-PipeSwitch-阅读笔记/"}}}