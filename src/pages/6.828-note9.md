---
title: 6.828 笔记9
date: 2019-03-06 12:20:00
tags: ["OS", "6.828"]
---

这里会记录阅读6.828课程lecture note的我的个人笔记。可能会中英混杂，不是很适合外人阅读，也请见谅。

## Lecture 10: Processes, threads, and scheduling

之前的一次作业基本上都是阅读代码和回答问题，所以就不单独列出来了。

### Process scheduling

什么是进程：

进程是an abstract virtual machine，仿佛其有自己的CPU和内存，并不受其他进程影响。主要是为了isolation。

进程的主要API有：

```c
  fork
  exec
  exit
  wait
  kill
  sbrk
  getpid
```

我们的挑战是很多时候，进程数比内核数多。这个时候我们就要用名叫time-sharing（分时）的方法，伴随以scheduling和context switch。

我们的主要目标是：

- transparent to user processes (kernel对于用户应用不可见)
- pre-emptive for user processes
- pre-emptive for kernel (帮助系统作响应)

xv6的解决方案是每个进程1个 user thread，1个kernel thread，每个处理器1个scheduler processor

什么是线程：

- a CPU core executing (with register and stack)
- a saved set of registers and a stack that could execute

xv6的进程切换的概况：

- user -> kernel thread (via system call or timer)
- kernel thread yields, due to pre-emption or waiting for I/O
- kernel thread -> scheduler thread
- scheduler thread finds a RUNNABLE kernel thread
- scheduler thread -> kernel thread
- kernel thread -> user

每个xv6 process都有一个状态`proc->state`，可以的取值为：

```c
  RUNNING
  RUNNABLE
  SLEEPING
  ZOMBIE
  UNUSED
```

注意：

- xv6有多个kernel thread，他们共享同一个kernel address space
- xv6的每个进程只有1个user thread
- 像Linux这样的系统支持没个进程多个线程。

context switching是xv6里最难做对的事了。

下面让我们来看看xv6的代码来学习一下它是怎么进行context swtich的：

我们没有讲义中提到的`hog.c`，所以没办法用gdb进行调试，不过还是可以跟着调试的路子看代码。

当开始发生context swtich的时候，会先通过时间中断触发`trap()`中的这部分：

```c
  // Force process to give up CPU on clock tick.
  // If interrupts were on while locks held, would need to check nlock.
  if(myproc() && myproc()->state == RUNNING &&
     tf->trapno == T_IRQ0+IRQ_TIMER)
    yield();
```

然后进入位于`proc.c`的`yield`：

```c
// Give up the CPU for one scheduling round.
void
yield(void)
{
  acquire(&ptable.lock);  //DOC: yieldlock
  myproc()->state = RUNNABLE;
  sched();
  release(&ptable.lock);
}
```

让当前进程等待之后，进入在同一个文件中的`shed()`

```c
// Enter scheduler.  Must hold only ptable.lock
// and have changed proc->state. Saves and restores
// intena because intena is a property of this
// kernel thread, not this CPU. It should
// be proc->intena and proc->ncli, but that would
// break in the few places where a lock is held but
// there's no process.
void
sched(void)
{
  int intena;
  struct proc *p = myproc();

  if(!holding(&ptable.lock))
    panic("sched ptable.lock");
  if(mycpu()->ncli != 1)
    panic("sched locks");
  if(p->state == RUNNING)
    panic("sched running");
  if(readeflags()&FL_IF)
    panic("sched interruptible");
  intena = mycpu()->intena;
  swtch(&p->context, mycpu()->scheduler);
  mycpu()->intena = intena;
}
```

之后就是`swtch()`函数，在`swtch.S`中：

```assembly
# Context switch
#
#   void swtch(struct context **old, struct context *new);
# 
# Save the current registers on the stack, creating
# a struct context, and save its address in *old.
# Switch stacks to new and pop previously-saved registers.

.globl swtch
swtch:
  movl 4(%esp), %eax  # &p->context
  movl 8(%esp), %edx  # mycpu()->scheduler

  # Save old callee-saved registers
  pushl %ebp
  pushl %ebx
  pushl %esi
  pushl %edi

  # Switch stacks
  movl %esp, (%eax)
  movl %edx, %esp

  # Load new callee-saved registers
  popl %edi
  popl %esi
  popl %ebx
  popl %ebp
  ret
```

保存切换前的一些寄存器，切换到了`mycpu()->scheduler`这个处理器的scheduler thread里，然后把切换后的环境的寄存器恢复回来，然后return。

scheduler thread里一直运行着`scheduler()`了。在`proc.c`中

```c
// Per-CPU process scheduler.
// Each CPU calls scheduler() after setting itself up.
// Scheduler never returns.  It loops, doing:
//  - choose a process to run
//  - swtch to start running that process
//  - eventually that process transfers control
//      via swtch back to the scheduler.
void
scheduler(void)
{
  struct proc *p;
  struct cpu *c = mycpu();
  c->proc = 0;
  
  for(;;){
    // Enable interrupts on this processor.
    sti();

    // Loop over process table looking for process to run.
    acquire(&ptable.lock);
    for(p = ptable.proc; p < &ptable.proc[NPROC]; p++){
      if(p->state != RUNNABLE)
        continue;

      // Switch to chosen process.  It is the process's job
      // to release ptable.lock and then reacquire it
      // before jumping back to us.
      c->proc = p;
      switchuvm(p);
      p->state = RUNNING;

      swtch(&(c->scheduler), p->context);
      switchkvm();

      // Process is done running for now.
      // It should have changed its p->state before coming back.
      c->proc = 0;
    }
    release(&ptable.lock);

  }
}
```

注意前面的`ptable.lock`和`yield`里是一个锁，就是为了在`yield`还没运行完之前不要运行`scheduler`里面的内容。对于下一个进程，调用`switchuvm()`。

```c
// Switch TSS and h/w page table to correspond to process p.
void
switchuvm(struct proc *p)
{
  if(p == 0)
    panic("switchuvm: no process");
  if(p->kstack == 0)
    panic("switchuvm: no kstack");
  if(p->pgdir == 0)
    panic("switchuvm: no pgdir");

  pushcli();
  mycpu()->gdt[SEG_TSS] = SEG16(STS_T32A, &mycpu()->ts,
                                sizeof(mycpu()->ts)-1, 0);
  mycpu()->gdt[SEG_TSS].s = 0;
  mycpu()->ts.ss0 = SEG_KDATA << 3;
  mycpu()->ts.esp0 = (uint)p->kstack + KSTACKSIZE;
  // setting IOPL=0 in eflags *and* iomb beyond the tss segment limit
  // forbids I/O instructions (e.g., inb and outb) from user space
  mycpu()->ts.iomb = (ushort) 0xFFFF;
  ltr(SEG_TSS << 3);
  lcr3(V2P(p->pgdir));  // switch to process's address space
  popcli();
}
```

`switchuvm`主要就是把栈的地址换好，再换好page directory，注意在这过程中开了中断了。回到`scheduler`，发现再进行下一次切换之前都不会做什么了。

下面是关于这整个步骤的几个问题：

- scheduling policy是什么：

  因为是循环运行，所以是Round Robin。除非只有两个进程在同时运行，刚刚`yield`的程序不会被马上运行。

- 为什么`scheduler`会在每个循环后会`release`，循环前会`acquire`。
